<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>NTSC‑RS Web Port</title>
  <style>
    /* Base styling */
    body {
      margin: 0;
      font-family: Arial, sans-serif;
      background-color: #0d1b2a;
      color: #f5f5f5;
      line-height: 1.6;
    }
    header {
      background: #1b263b;
      padding: 20px;
      text-align: center;
    }
    header h1 {
      margin: 0;
      font-size: 2em;
    }
    .container {
      max-width: 900px;
      margin: 30px auto;
      padding: 0 15px;
    }
    #videoArea {
      position: relative;
      width: 640px;
      height: 480px;
      margin: 0 auto;
      border: 4px solid #415a77;
      box-shadow: 0 0 10px rgba(0, 0, 0, 0.7);
    }
    #displayCanvas {
      width: 100%;
      height: 100%;
      cursor: ns-resize; /* indicate vertical adjustment */
    }
    .controls {
      margin-top: 15px;
      text-align: center;
    }
    .controls label {
      margin-right: 10px;
    }
    #intensity-indicator {
      margin-top: 10px;
      text-align: center;
      font-size: 0.9em;
      color: #aabec6;
    }
    button {
      padding: 8px 12px;
      margin-right: 10px;
      background: #778da9;
      border: none;
      border-radius: 4px;
      color: #fff;
      cursor: pointer;
    }
    button:disabled {
      opacity: 0.5;
      cursor: not-allowed;
    }
  </style>
</head>
<body>
  <header>
    <h1>NTSC‑RS Web Port</h1>
    <p>A browser‑based VHS/NTSC simulator</p>
  </header>
  <div class="container">
    <div class="controls">
      <label>
        Upload video/image:
        <input type="file" id="fileInput" accept="video/*,image/*" />
      </label>
      <button id="startCamera">Use Camera</button>
      <button id="recordBtn" disabled>Start Recording</button>
    </div>
    <div id="videoArea">
      <!-- Hidden media elements -->
      <video id="video" playsinline style="display:none"></video>
      <canvas id="sourceCanvas" width="640" height="480" style="display:none"></canvas>
      <!-- Display canvas for processed output -->
      <canvas id="displayCanvas" width="640" height="480"></canvas>
    </div>
    <div class="controls">
      <label><input type="checkbox" id="toggleScan" checked /> Scanlines</label>
      <label><input type="checkbox" id="toggleColor" checked /> Color bleed</label>
      <label><input type="checkbox" id="toggleNoise" checked /> Static noise</label>
      <label><input type="checkbox" id="toggleJitter" checked /> Frame jitter</label>
      <label><input type="checkbox" id="toggleDrop" checked /> Dropout lines</label>
    </div>
    <div id="intensity-indicator">Intensity: 100%</div>
  </div>
  <script>
    // DOM references
    const fileInput = document.getElementById('fileInput');
    const startCameraBtn = document.getElementById('startCamera');
    const recordBtn = document.getElementById('recordBtn');
    const video = document.getElementById('video');
    const sourceCanvas = document.getElementById('sourceCanvas');
    const displayCanvas = document.getElementById('displayCanvas');
    const sourceCtx = sourceCanvas.getContext('2d');
    const displayCtx = displayCanvas.getContext('2d');
    const intensityIndicator = document.getElementById('intensity-indicator');
    const controls = {
      scan: document.getElementById('toggleScan'),
      color: document.getElementById('toggleColor'),
      noise: document.getElementById('toggleNoise'),
      jitter: document.getElementById('toggleJitter'),
      drop: document.getElementById('toggleDrop'),
    };

    let intensity = 1.0; // Global intensity factor
    let recording = false;
    let mediaRecorder = null;
    let recordedChunks = [];
    let animationFrameId = null;
    let currentSource = null; // 'video', 'image', or 'camera'

    // Update intensity based on vertical position over display canvas
    function updateIntensityFromPosition(y) {
      const rect = displayCanvas.getBoundingClientRect();
      const relativeY = Math.max(0, Math.min(y - rect.top, rect.height));
      const ratio = 1 - (relativeY / rect.height);
      // intensity ranges 0.5 to 2.0
      intensity = 0.5 + (1.5 * ratio);
      intensityIndicator.textContent = `Intensity: ${Math.round(intensity * 100)}%`;
    }

    displayCanvas.addEventListener('mousemove', (e) => {
      if (e.buttons === 1 || e.buttons === 0) {
        updateIntensityFromPosition(e.clientY);
      }
    });
    displayCanvas.addEventListener('touchmove', (e) => {
      if (e.touches.length) {
        updateIntensityFromPosition(e.touches[0].clientY);
      }
    });

    // File input handler
    fileInput.addEventListener('change', async (e) => {
      const file = e.target.files[0];
      if (!file) return;
      stopCurrentSource();
      const url = URL.createObjectURL(file);
      if (file.type.startsWith('video')) {
        currentSource = 'video';
        video.srcObject = null;
        video.src = url;
        video.loop = true;
        await video.play();
        processVideoFrame();
      } else if (file.type.startsWith('image')) {
        currentSource = 'image';
        const img = new Image();
        img.onload = () => {
          sourceCanvas.width = img.width;
          sourceCanvas.height = img.height;
          displayCanvas.width = img.width;
          displayCanvas.height = img.height;
          sourceCtx.drawImage(img, 0, 0);
          processImageFrame();
        };
        img.src = url;
      }
    });

    // Start camera
    startCameraBtn.addEventListener('click', async () => {
      stopCurrentSource();
      currentSource = 'camera';
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ video: true, audio: false });
        video.srcObject = stream;
        await video.play();
        processVideoFrame();
      } catch (err) {
        alert('Webcam access denied or unavailable.');
        console.error(err);
      }
    });

    // Recording handler
    recordBtn.addEventListener('click', () => {
      if (!recording) {
        // Start recording
        const stream = displayCanvas.captureStream(30);
        mediaRecorder = new MediaRecorder(stream, { mimeType: 'video/webm' });
        recordedChunks = [];
        mediaRecorder.ondataavailable = (e) => {
          if (e.data.size > 0) recordedChunks.push(e.data);
        };
        mediaRecorder.onstop = () => {
          const blob = new Blob(recordedChunks, { type: 'video/webm' });
          const url = URL.createObjectURL(blob);
          const a = document.createElement('a');
          a.href = url;
          a.download = 'ntsc-rs-recording.webm';
          a.click();
        };
        mediaRecorder.start();
        recording = true;
        recordBtn.textContent = 'Stop Recording';
      } else {
        // Stop recording
        mediaRecorder.stop();
        recording = false;
        recordBtn.textContent = 'Start Recording';
      }
    });

    // Stop any current media source and cancel animation
    function stopCurrentSource() {
      if (animationFrameId) cancelAnimationFrame(animationFrameId);
      if (video.srcObject) {
        // stop camera
        const tracks = video.srcObject.getTracks();
        tracks.forEach((t) => t.stop());
        video.srcObject = null;
      }
      video.pause();
      video.src = '';
      recordBtn.disabled = true;
    }

    // Process image (single frame)
    function processImageFrame() {
      const frame = sourceCtx.getImageData(0, 0, sourceCanvas.width, sourceCanvas.height);
      const output = applyNTSC(frame);
      displayCtx.putImageData(output, 0, 0);
      recordBtn.disabled = false;
    }

    // Process video/camera frames in a loop
    function processVideoFrame() {
      if (!video.paused && !video.ended) {
        // Resize canvases to video frame size
        if (sourceCanvas.width !== video.videoWidth || sourceCanvas.height !== video.videoHeight) {
          sourceCanvas.width = video.videoWidth;
          sourceCanvas.height = video.videoHeight;
          displayCanvas.width = video.videoWidth;
          displayCanvas.height = video.videoHeight;
        }
        sourceCtx.drawImage(video, 0, 0, sourceCanvas.width, sourceCanvas.height);
        const frame = sourceCtx.getImageData(0, 0, sourceCanvas.width, sourceCanvas.height);
        const output = applyNTSC(frame);
        displayCtx.putImageData(output, 0, 0);
        recordBtn.disabled = false;
      }
      animationFrameId = requestAnimationFrame(processVideoFrame);
    }

    // Apply NTSC/VHS effects to an ImageData object and return new ImageData.
    // This implementation more closely models the analog NTSC pipeline by converting
    // RGB to YIQ, modulating the I/Q channels onto a color subcarrier, then
    // demodulating and low‑pass filtering back to RGB. Additional optional
    // artifacts (scanlines, noise, jitter, dropouts) can be toggled via the
    // controls object.
    function applyNTSC(frame) {
      const src = frame.data;
      const width = frame.width;
      const height = frame.height;
      // Destination buffer
      const out = new Uint8ClampedArray(width * height * 4);
      // Preallocate per‑row arrays to avoid reallocations inside loops
      const YArr = new Float32Array(width);
      const IArr = new Float32Array(width);
      const QArr = new Float32Array(width);
      const comp = new Float32Array(width);
      const IMod = new Float32Array(width);
      const QMod = new Float32Array(width);
      // Precompute subcarrier frequency factor based on image width. NTSC has
      // ~227.5 color subcarrier cycles per line, so we compute cycles per pixel.
      const twoPI = 2 * Math.PI;
      const freqFactor = 227.5 / width;
      // Iterate over each scanline
      for (let y = 0; y < height; y++) {
        // Random phase jitter to simulate color carrier wobble
        let phaseJitter = 0;
        if (controls.jitter.checked) {
          phaseJitter = (Math.random() - 0.5) * 0.4 * intensity;
        }
        // Convert RGB to YIQ and build composite signal
        for (let x = 0; x < width; x++) {
          const idx = (y * width + x) * 4;
          // Normalized RGB in range [0,1]
          let R = src[idx] / 255;
          let G = src[idx + 1] / 255;
          let B = src[idx + 2] / 255;
          // Optional additive static noise on RGB before conversion
          if (controls.noise.checked) {
            const n = (Math.random() - 0.5) * 0.1 * intensity;
            R = Math.min(1, Math.max(0, R + n));
            G = Math.min(1, Math.max(0, G + n));
            B = Math.min(1, Math.max(0, B + n));
          }
          // Convert to YIQ
          const Yval = 0.299 * R + 0.587 * G + 0.114 * B;
          let Ival = 0.596 * R - 0.274 * G - 0.322 * B;
          let Qval = 0.211 * R - 0.522 * G + 0.311 * B;
          // Reduce chroma amplitude to avoid unrealistic oversaturation
          const chromaScale = controls.color.checked ? (0.8 * intensity) : 0.0;
          Ival *= chromaScale;
          Qval *= chromaScale;
          YArr[x] = Yval;
          IArr[x] = Ival;
          QArr[x] = Qval;
          // Determine subcarrier angle: approx 227.5 cycles per line
          // Add phaseJitter for dynamic wobble
          if (controls.color.checked) {
            const angle = twoPI * (x * freqFactor + phaseJitter);
            comp[x] = Yval + Ival * Math.cos(angle) + Qval * Math.sin(angle);
          } else {
            // If color bleed disabled, just copy luma to composite
            comp[x] = Yval;
          }
        }
        // Demodulate chroma channels from composite
        for (let x = 0; x < width; x++) {
          if (controls.color.checked) {
            const angle = twoPI * (x * freqFactor + phaseJitter);
            IMod[x] = comp[x] * Math.cos(angle);
            QMod[x] = comp[x] * Math.sin(angle);
          } else {
            IMod[x] = 0;
            QMod[x] = 0;
          }
        }
        // Simple low‑pass filter (sliding average) to simulate finite analog bandwidth
        for (let x = 0; x < width; x++) {
          let sumY = 0;
          let sumI = 0;
          let sumQ = 0;
          let count = 0;
          for (let k = -2; k <= 2; k++) {
            const xi = x + k;
            if (xi >= 0 && xi < width) {
              sumY += comp[xi];
              sumI += IMod[xi];
              sumQ += QMod[xi];
              count++;
            }
          }
          const Yf = sumY / count;
          // Multiply by 2 per demodulation theory (original subcarrier suppressed)
          const If = (sumI / count) * 2;
          const Qf = (sumQ / count) * 2;
          // Convert back to RGB
          let r = Yf + 0.956 * If + 0.621 * Qf;
          let g = Yf - 0.272 * If - 0.647 * Qf;
          let b = Yf - 1.106 * If + 1.703 * Qf;
          // Apply optional scanlines (darken every other line)
          if (controls.scan.checked && (y % 2 === 0)) {
            r *= 0.8;
            g *= 0.8;
            b *= 0.8;
          }
          // Apply dropout lines: occasionally blank segments
          if (controls.drop.checked && Math.random() < 0.0005 * intensity) {
            r = g = b = 0;
          }
          // Clamp to [0,1]
          r = Math.min(1, Math.max(0, r));
          g = Math.min(1, Math.max(0, g));
          b = Math.min(1, Math.max(0, b));
          const outIdx = (y * width + x) * 4;
          out[outIdx] = r * 255;
          out[outIdx + 1] = g * 255;
          out[outIdx + 2] = b * 255;
          out[outIdx + 3] = 255;
        }
      }
      return new ImageData(out, width, height);
    }

    // When page is unloaded, stop any media
    window.addEventListener('beforeunload', () => {
      stopCurrentSource();
    });
  </script>
</body>
</html>